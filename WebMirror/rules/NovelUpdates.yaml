

FOLLOW_GOOGLE_LINKS: False
allImages: False

type: "eastern"

baseUrl: [
	"http://www.novelupdates.com",
	]

send_raw_feed: False
trigger: False

badwords: [
			"/about/",
			"/join-us/",
			"/chat/",
			'&format=pdf',
			'?format=pdf',
			'?replytocom=',
			"/forum/",
			"/forum",
			"/forums/",
			"#comment",
			"/forums",

			"/post.php?",
			"/author.php?",
			'/more-reviews/',
			'/readlist/',
			'/extnu/',

			'/wp-content/themes/',
			'/login/?redirect_to',
			'/wp-content/uploads/',


			# Don't actually spider the site, just let the update triggers
			# do their thing.
			'www.novelupdates.com',
			]

# Content Stripping needs to be determined.
decomposeBefore: [

]

decompose: [
]

stripTitle: []


destyle : [
]


preserveAttrs : [
]

special_case_filters : {

	# Apparently they don't like bots. Well, too bad.
	"www.novelupdates.com" : ["remote_fetch", "NUWebRequest", "getItemPhantomJS"],
	"novelupdates.com"     : ["remote_fetch", "NUWebRequest", "getItemPhantomJS"],

	# So I can test against someone less bot-sensitive
	"www.google.com" : ["remote_fetch", "NUWebRequest", "getItemPhantomJS"],
}
